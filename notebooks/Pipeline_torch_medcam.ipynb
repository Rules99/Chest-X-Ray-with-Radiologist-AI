{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torchxrayvision.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Torchxrayvision \n",
        "Torchxrayvision is a platform based on pretrained models from chest x ray radiographies. The purpose ofthis notebook is managing the torchxrayvision framewrl"
      ],
      "metadata": {
        "id": "KQHMnY_XkgZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture \n",
        "!pip install torchxrayvision\n",
        "from google.colab import drive\n",
        "import os \n",
        "import torchxrayvision as xrv"
      ],
      "metadata": {
        "id": "iZJIz40zrIQ5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive\n",
        "os.chdir('./TFG/torchxrayvision/')\n",
        "base_path = os.getcwd()\n",
        "base_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ioV5Om9-seQZ",
        "outputId": "20d1d39a-983c-4e7e-9e67-cd64761a74c1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/TFG/torchxrayvision'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(base_path+\"/data\")"
      ],
      "metadata": {
        "id": "EWQ7RPUSzKWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c8c2966-85ac-4d24-ba0f-4eedeb007957"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RALO-Dataset.zip',\n",
              " '.ipynb_checkpoints',\n",
              " 'volumes.zip',\n",
              " 'metadata.csv',\n",
              " 'NLMCXR_png.zip']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from zipfile import ZipFile\n",
        "\n",
        "# with ZipFile(f'{base_path}/data/NLMCXR_png.zip', 'r') as zipObj:\n",
        "#    # Extract all the contents of zip file in current directory\n",
        "#    zipObj.extractall('NLM')"
      ],
      "metadata": {
        "id": "Bladgd8B5ded"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.Models \n",
        "The models are already pretrained with it corresponding classifiers. To access to models we have modules in torchxrayvision to load pretrained models in pytorch"
      ],
      "metadata": {
        "id": "lcM39pJhkzCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generic function to load any core model\n",
        "model = xrv.models.get_model(weights=\"densenet121-res224-all\")\n",
        "# # DenseNet 224x224 model trained on multiple datasets\n",
        "# model = xrv.models.DenseNet(weights=\"densenet121-res224-all\")\n",
        "# # DenseNet trained on just the RSNA Pneumonia dataset\n",
        "# model = xrv.models.DenseNet(weights=\"densenet121-res224-rsna\")\n",
        "# # ResNet 512x512 model trained on multiple datasets\n",
        "# model = xrv.models.ResNet(weights=\"resnet50-res512-all\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug_nsNLukwIH",
        "outputId": "21a81c4c-ae41-4f1a-fa4e-ab43cb647e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading weights...\n",
            "If this fails you can run `wget https://github.com/mlmed/torchxrayvision/releases/download/v1/nih-pc-chex-mimic_ch-google-openi-kaggle-densenet121-d121-tw-lr001-rot45-tr15-sc15-seed0-best.pt -O /root/.torchxrayvision/models_data/nih-pc-chex-mimic_ch-google-openi-kaggle-densenet121-d121-tw-lr001-rot45-tr15-sc15-seed0-best.pt`\n",
            "[██████████████████████████████████████████████████]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 Baseline classifiers \n",
        "There are different model classifiers according to the purpose you want to predict"
      ],
      "metadata": {
        "id": "rAE8UIVslTne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DenseNet121 from JF Healthcare for the CheXpert competition\n",
        "model = xrv.baseline_models.jfhealthcare.DenseNet()\n",
        "# # Official Stanford CheXpert model\n",
        "# model = xrv.baseline_models.chexpert.DenseNet()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "H3Q83rGBlEWw",
        "outputId": "78aa9e86-5c85-497b-f15c-856585225aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-75280ae1564e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# model = xrv.baseline_models.jfhealthcare.DenseNet()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Official Stanford CheXpert model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxrv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseline_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchexpert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDenseNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchxrayvision/baseline_models/chexpert/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weights_zip, num_models)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights_zip\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Need to specify weights_zip file location. You can download them from {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Need to specify weights_zip file location. You can download them from https://academictorrents.com/details/5c7ee21e6770308f2d2b4bd829e896dbd9d3ee87"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 Classifier interface\n",
        "We have a method to predict the classifications of the output label"
      ],
      "metadata": {
        "id": "DPRMFD3pmIyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# img = 'Choose an image to classify the labels'\n",
        "# predictions = model(img)[0] # 0 is first element of batch\n",
        "# dict(zip(model.pathologies,predictions.detach().numpy()))\n",
        "# prediction = model(img)[:,model.pathologies.index(\"Consolidation\")]\n"
      ],
      "metadata": {
        "id": "nbTNoX9ClS07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3 Feature Extraction \n",
        "We also have a method to interpret the different features of the model and labels with UMAP visualizations"
      ],
      "metadata": {
        "id": "Z14Tfen-meZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feats = model.features(img)\n"
      ],
      "metadata": {
        "id": "L8nDUIpzmvxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4 Autoencoders \n",
        "We have a pretrained autoencoder network to generate synthethic data from certain distribution. We can encode the image onto a distribution and decode a distribution to get an chest x ray image\n"
      ],
      "metadata": {
        "id": "OcY_yUpPm13j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# image = 'Image to select '\n",
        "# ae = xrv.autoencoders.ResNetAE(weights=\"101-elastic\")\n",
        "# z = ae.encode(image)\n",
        "# image2 = ae.decode(z)\n"
      ],
      "metadata": {
        "id": "OSHwcU7um0vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Datasets \n",
        "The datsets if the torchzrayvision"
      ],
      "metadata": {
        "id": "TEyaEKTMnHeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This is a dataset with Geographic Extend and Lung Opacity Features\n",
        "'''\n",
        "dataset = xrv.datasets.StonyBrookCOVID_Dataset(imgpath=base_path+'/StonyBrook/imgs',\n",
        "csvpath=base_path+'/StonyBrook/data.csv')\n",
        "'''\n",
        "Covid19 Dataset\n",
        "'''\n",
        "dataset2 = xrv.datasets.COVID19_Dataset(imgpath=f'{base_path}/data/volumes.zip',csvpath=f'{base_path}/data/metadata.csv')"
      ],
      "metadata": {
        "id": "ySJ585yqnNDL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.path.join(base_path,'/data/NLM/NLMCXR_png.tgz')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3By5EEvfK6nC",
        "outputId": "61c88ac5-cd56-435d-cf28-5522f449775e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/data/NLM/NLMCXR_png.tgz'"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "dataset3 = xrv.datasets.Openi_Dataset( os.path.join(base_path,'/data/NLM/NLMCXR_png.tgz'),xmlpath=os.path.join(base_path,'NLMCXR_reports.tgz'))"
      ],
      "metadata": {
        "id": "LOAEHH2F43cr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MedCam Visualization with images\n",
        "Medcam is a library implemented in pytorch used to visualize medical images, maybe a trial will be useful to consider if the library works or not"
      ],
      "metadata": {
        "id": "CJ4UhqlO7gqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install medcam"
      ],
      "metadata": {
        "id": "4zS9Lv1u7f4B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import cv2\n",
        "\n",
        "# Setup the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = xrv.models.get_model(weights=\"densenet121-res224-all\")\n",
        "model.to(device=device)\n",
        "model.eval()\n",
        "\n",
        "def load_image(image_path):\n",
        "    raw_image = cv2.imread(image_path)\n",
        "    raw_image = cv2.resize(raw_image, (224,) * 2)\n",
        "    image = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ]\n",
        "    )(raw_image[..., ::-1].copy())\n",
        "    image = image.to(device)\n",
        "    return image\n",
        "\n",
        "# Load the dataset\n",
        "\n",
        "'''\n",
        "You must labels the images according to each of the true target it has \n",
        "\n",
        "'''\n",
        "dataset = ImageFolder(f'NLM2/', loader=load_image)\n",
        "# Set up the dataloader\n",
        "'''\n",
        "Load the dataset\n",
        "'''\n",
        "data_loader = DataLoader(dataset, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "GX7NUNqQ7odZ",
        "outputId": "b2396846-c917-4171-d3d6-f8544d0fe182"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b8c1aa7ee478>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{base_path}/StonyBrook/imgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;31m# Set up the dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    311\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    143\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m    144\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in /content/drive/MyDrive/TFG/torchxrayvision/StonyBrook/imgs."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Called to the MedCam Library so that \n",
        "loads the visualization\n",
        "In output dir the module outputs the image with gradcam, c2d or whatever backend you use\n",
        "'''\n",
        "from medcam import medcam\n",
        "\n",
        "model = medcam.inject(model, output_dir='attention_maps', backend='gcam', layer='layer4', label='best', save_maps=True)"
      ],
      "metadata": {
        "id": "0j6dFbWFDYF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nHb5hbNIDpOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os, sys, tarfile\n",
        "\n",
        "# def extract(tar_url, extract_path='.'):\n",
        "#     tar = tarfile.open(tar_url, 'r')\n",
        "#     for item in tar:\n",
        "#         tar.extract(item, extract_path)\n",
        "#         if item.name.find(\".tgz\") != -1 or item.name.find(\".tar\") != -1:\n",
        "#             extract(item.name, \"./\" + item.name[:item.name.rfind('/')],extract_path)\n",
        "\n",
        "\n",
        "# extract(f'{base_path}/NLM/NLMCXR_png.tgz',extract_path='NLM2')\n",
        "\n",
        "# except:\n",
        "#     name = os.path.basename(sys.argv[0])\n",
        "#     print name[:name.rfind('.')], '<filename>'\n"
      ],
      "metadata": {
        "id": "LNgxCOQB6icE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}